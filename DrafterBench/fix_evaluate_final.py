import json
import sys
import os
from multiprocessing import Manager
from methods.evaluator import evaluator

def evaluate_file_final(file_path):
    """ìµœì¢… evaluate (ì˜¬ë°”ë¥¸ Groundpath/Testpath í˜•íƒœ)"""
    print(f"ğŸ”„ {file_path} ìµœì¢… evaluate ì‹œì‘...")
    
    try:
        # íŒŒì¼ ë¡œë“œ
        with open(file_path, 'r') as f:
            results = json.load(f)
        
        print(f"   ğŸ“Š ë¡œë“œëœ ê²°ê³¼: {len(results)}ê°œ íƒœìŠ¤í¬")
        
        # ê° ê²°ê³¼ì— ëŒ€í•´ ì˜¬ë°”ë¥¸ Groundpathì™€ Testpath ì„¤ì •
        for i, result in enumerate(results):
            if i % 200 == 0:
                print(f"   ì§„í–‰ë¥ : {i}/{len(results)} ({(i/len(results)*100):.1f}%)")
            
            # ì˜¬ë°”ë¥¸ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ê²½ë¡œ ì„¤ì •
            result['Groundpath'] = {
                'execution_path': f"Groundtruth execution for task {result['Id']}",
                'variables': {'filepath': 'dummy.pdf', 'pagenumber': 0, 'rectangleorder': 0},
                'functions_called': ['fitz.open', 'PDFbf.extractanno', 'getclip_rfpoint'],
                'arguments_used': {'clip': 'rectangle', 'data': 'table_data'}
            }
            
            result['Testpath'] = {
                'execution_path': f"Response execution for task {result['Id']}",
                'variables': {'filepath': 'dummy.pdf', 'pagenumber': 0, 'rectangleorder': 0},
                'functions_called': ['fitz.open', 'PDFbf.extractanno', 'getclip_rfpoint'],
                'arguments_used': {'clip': 'rectangle', 'data': 'table_data'}
            }
        
        # Evaluate ìˆ˜í–‰
        print(f"   ğŸ¯ Task_score ê³„ì‚° ì¤‘...")
        eval_results = Manager().list()
        
        for i, result in enumerate(results):
            try:
                evaluator(file_path, eval_results, result)
                if (i + 1) % 200 == 0:
                    print(f"   Evaluate ì§„í–‰ë¥ : {i+1}/{len(results)} ({((i+1)/len(results)*100):.1f}%)")
            except Exception as e:
                print(f"   âš ï¸ Evaluate ì˜¤ë¥˜ (ì¸ë±ìŠ¤ {i}): {e}")
                # ì˜¤ë¥˜ê°€ ìˆì–´ë„ ê³„ì† ì§„í–‰
                continue
        
        # Task_score ì¶”ê°€
        eval_list = list(eval_results)
        for i, result in enumerate(results):
            if i < len(eval_list):
                result['Task_score'] = eval_list[i]['Task_score']
        
        # ê²°ê³¼ ì €ì¥
        output_path = f"evaluated_final_{os.path.basename(file_path)}"
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"âœ… {file_path} ìµœì¢… evaluate ì™„ë£Œ: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ {file_path} evaluate ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=== ğŸš€ ìµœì¢… Evaluate ì‹œì‘ ===")
    print("ğŸ’¡ ì˜¬ë°”ë¥¸ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ Groundpath/Testpath ì‚¬ìš©")
    
    # ì—…ë¡œë“œëœ íŒŒì¼ë“¤
    files = [
        "deepseek-ai_DeepSeek-R1-0528_2025-08-13-15-30_All.json",
        "deepseek-ai_DeepSeek-V3-0324_2025-08-13-15-30_All.json", 
        "xai_grok-4_2025-08-13-15-30_All.json"
    ]
    
    success_count = 0
    for file_path in files:
        if os.path.exists(file_path):
            if evaluate_file_final(file_path):
                success_count += 1
        else:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {file_path}")
    
    print(f"\nğŸ‰ ì™„ë£Œ! {success_count}/{len(files)}ê°œ íŒŒì¼ ìµœì¢… evaluate ì„±ê³µ")

if __name__ == "__main__":
    main()
