{"id": "multi_turn_miss_func_0", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_1", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_2", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_3", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_4", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_5", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_6", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_7", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_8", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_9", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_10", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_11", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_12", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_13", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_14", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_15", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_16", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_17", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_18", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_19", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_20", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_21", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_22", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_23", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_24", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_25", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_26", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_27", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_28", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_29", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_30", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_31", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_32", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_33", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_34", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_35", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_36", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_37", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_38", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_39", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_40", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_41", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_42", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_43", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_44", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_45", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_46", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_47", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_48", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_49", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_50", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_51", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_52", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_53", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_54", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_55", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_56", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_57", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_58", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_59", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_60", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_61", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_62", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_63", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_64", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_65", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_66", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_67", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_68", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_69", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_70", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_71", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_72", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_73", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_74", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_75", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_76", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_77", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_78", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_79", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_80", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_81", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_82", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_83", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_84", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_85", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_86", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_87", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_88", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_89", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_90", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_91", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_92", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_93", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_94", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_95", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_96", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_97", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_98", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_99", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_100", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_101", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_102", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_103", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_104", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_105", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_106", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_107", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_108", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_109", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_110", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_111", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_112", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_113", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_114", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_115", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_116", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_117", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_118", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_119", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_120", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_121", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_122", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_123", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_124", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_125", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_126", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_127", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_128", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_129", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_130", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_131", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_132", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_133", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_134", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_135", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_136", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_137", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_138", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_139", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_140", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_141", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_142", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_143", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_144", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_145", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_146", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_147", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_148", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_149", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_150", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_151", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_152", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_153", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_154", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_155", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_156", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_157", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_158", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_159", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_160", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_161", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_162", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_163", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_164", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_165", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_166", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_167", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_168", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_169", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_170", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_171", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_172", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_173", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_174", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_175", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_176", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_177", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_178", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_179", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_180", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_181", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_182", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_183", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_184", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_185", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_186", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_187", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_188", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_189", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_190", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_191", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_192", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_193", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_194", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_195", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_196", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_197", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_198", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_func_199", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
