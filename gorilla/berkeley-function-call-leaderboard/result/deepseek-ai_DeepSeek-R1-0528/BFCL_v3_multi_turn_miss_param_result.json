{"id": "multi_turn_miss_param_0", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_1", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_2", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_3", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_4", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_5", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_6", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_7", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_8", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_9", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_10", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_11", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_12", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_13", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_14", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_15", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_16", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_17", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_18", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_19", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_20", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_21", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_22", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_23", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_24", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_25", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_26", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_27", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_28", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_29", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_30", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_31", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_32", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_33", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_34", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_35", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_36", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_37", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_38", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_39", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_40", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_41", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_42", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_43", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_44", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_45", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_46", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_47", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_48", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_49", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_50", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_51", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_52", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_53", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_54", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_55", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_56", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_57", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_58", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_59", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_60", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_61", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_62", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_63", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_64", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_65", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_66", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_67", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_68", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_69", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_70", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_71", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_72", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_73", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_74", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_75", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_76", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_77", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_78", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_79", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_80", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_81", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_82", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_83", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_84", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_85", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_86", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_87", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_88", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_89", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_90", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_91", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_92", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_93", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_94", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_95", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_96", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_97", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_98", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_99", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_100", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_101", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_102", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_103", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_104", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_105", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_106", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_107", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_108", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_109", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_110", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_111", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_112", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_113", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_114", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_115", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_116", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_117", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_118", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_119", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_120", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_121", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_122", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_123", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_124", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_125", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_126", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_127", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_128", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_129", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_130", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_131", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_132", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_133", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_134", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_135", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_136", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_137", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_138", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_139", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_140", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_141", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_142", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_143", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_144", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_145", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_146", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_147", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_148", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_149", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_150", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_151", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_152", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_153", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_154", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_155", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_156", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_157", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_158", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_159", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_160", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_161", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_162", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_163", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_164", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_165", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_166", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_167", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_168", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_169", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_170", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_171", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_172", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_173", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_174", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_175", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_176", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_177", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_178", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_179", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_180", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_181", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_182", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_183", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_184", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_185", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_186", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_187", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_188", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_189", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_190", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_191", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_192", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_193", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_194", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_195", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_196", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_197", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_198", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_miss_param_199", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
