{"id": "multi_turn_base_0", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_1", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_2", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_3", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_4", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_5", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_6", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_7", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_8", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_9", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_10", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_11", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_12", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_13", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_14", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_15", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_16", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_17", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_18", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_19", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_20", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_21", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_22", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_23", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_24", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_25", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_26", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_27", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_28", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_29", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_30", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_31", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_32", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_33", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_34", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_35", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_36", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_37", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_38", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_39", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_40", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_41", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_42", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_43", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_44", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_45", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_46", "result": [["To delete all files in the 'Drafts' directory and the directory itself, follow these steps:\n\n1. **Remove all files and subdirectories inside 'Drafts'**:\n   - Use the `find` tool to list all items under 'Drafts'.\n   - Process each item with `rm` to ensure everything is deleted.\n\n2. **Delete the 'Drafts' directory**:\n   - After emptying 'Drafts', use `rmdir` to remove the directory itself.\n\nHere's the sequence of commands:\n\n```python\n# Step 1: Find and list all items in 'Drafts'\nitems = find(path=\"Drafts\", name=None)\n\n# Step 2: Delete each item in 'Drafts' (including nested files/directories)\nfor item in items:\n    rm(item)\n\n# Step 3: Remove the empty 'Drafts' directory\nrmdir(\"Drafts\")\n```\n\n### Explanation:\n- **`find(path=\"Drafts\", name=None)`**: Lists every file and subdirectory under 'Drafts'.\n- **`rm(item)`**: Deletes each item found in 'Drafts'.\n- **`rmdir(\"Drafts\")`**: Removes the now-empty 'Drafts' directory.\n\nThis ensures 'Drafts' and all its contents are permanently deleted."]], "input_token_count": [[4593]], "output_token_count": [[9663]], "latency": [[517.6537396907806]], "inference_log": [[{"role": "state_info", "class_name": "MessageAPI", "content": {"generated_ids": "set()", "user_count": 4, "user_map": {"Alice": "USR001", "Bob": "USR002", "Catherine": "USR003", "Daniel": "USR004"}, "inbox": [{"USR002": "My name is Alice. I want to connect."}, {"USR003": "Could you upload the file?"}, {"USR004": "Could you upload the file?"}], "message_count": 3, "current_user": null}}, {"role": "state_info", "class_name": "GorillaFileSystem", "content": {"long_context": false, "root": "<Directory: dylan, Parent: None, Contents: {'Drafts': <Directory: Drafts, Parent: dylan, Contents: {'DylanProject.txt': <<File: DylanProject.txt, Content: Initial outline of the Dylan project.>>}>, 'ArchivedProjects': <Directory: ArchivedProjects, Parent: dylan, Contents: {}>}>"}}], {"begin_of_turn_query": [{"role": "user", "content": "Delete all the files in the 'Drafts' directory including the directory."}], "step_0": [{"role": "assistant", "content": "To delete all files in the 'Drafts' directory and the directory itself, follow these steps:\n\n1. **Remove all files and subdirectories inside 'Drafts'**:\n   - Use the `find` tool to list all items under 'Drafts'.\n   - Process each item with `rm` to ensure everything is deleted.\n\n2. **Delete the 'Drafts' directory**:\n   - After emptying 'Drafts', use `rmdir` to remove the directory itself.\n\nHere's the sequence of commands:\n\n```python\n# Step 1: Find and list all items in 'Drafts'\nitems = find(path=\"Drafts\", name=None)\n\n# Step 2: Delete each item in 'Drafts' (including nested files/directories)\nfor item in items:\n    rm(item)\n\n# Step 3: Remove the empty 'Drafts' directory\nrmdir(\"Drafts\")\n```\n\n### Explanation:\n- **`find(path=\"Drafts\", name=None)`**: Lists every file and subdirectory under 'Drafts'.\n- **`rm(item)`**: Deletes each item found in 'Drafts'.\n- **`rmdir(\"Drafts\")`**: Removes the now-empty 'Drafts' directory.\n\nThis ensures 'Drafts' and all its contents are permanently deleted."}, {"role": "handler_log", "content": "Error decoding the model response. Proceed to next turn.", "error": "'str' object has no attribute 'items'"}]}, [{"role": "state_info", "class_name": "MessageAPI", "content": {"generated_ids": "set()", "user_count": 4, "user_map": {"Alice": "USR001", "Bob": "USR002", "Catherine": "USR003", "Daniel": "USR004"}, "inbox": [{"USR002": "My name is Alice. I want to connect."}, {"USR003": "Could you upload the file?"}, {"USR004": "Could you upload the file?"}], "message_count": 3, "current_user": null}}, {"role": "state_info", "class_name": "GorillaFileSystem", "content": {"long_context": false, "root": "<Directory: dylan, Parent: None, Contents: {'Drafts': <Directory: Drafts, Parent: dylan, Contents: {'DylanProject.txt': <<File: DylanProject.txt, Content: Initial outline of the Dylan project.>>}>, 'ArchivedProjects': <Directory: ArchivedProjects, Parent: dylan, Contents: {}>}>"}}]]}
{"id": "multi_turn_base_47", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_48", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_49", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_50", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_51", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_52", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_53", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_54", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_55", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_56", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_57", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_58", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_59", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_60", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_61", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_62", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_63", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_64", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_65", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_66", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_67", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_68", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_69", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_70", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_71", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_72", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_73", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_74", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_75", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_76", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_77", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_78", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_79", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_80", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_81", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_82", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_83", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_84", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_85", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_86", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_87", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_88", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_89", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_90", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_91", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_92", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_93", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_94", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_95", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_96", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_97", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_98", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_99", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_100", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_101", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_102", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_103", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_104", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_105", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_106", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_107", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_108", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_109", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_110", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_111", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_112", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_113", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_114", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_115", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_116", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_117", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_118", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_119", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_120", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_121", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_122", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_123", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_124", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_125", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_126", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_127", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_128", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_129", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_130", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_131", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_132", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_133", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_134", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_135", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_136", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_137", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_138", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_139", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_140", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_141", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_142", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_143", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_144", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_145", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_146", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_147", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_148", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_149", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_150", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_151", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_152", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_153", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_154", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_155", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_156", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_157", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_158", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_159", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_160", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_161", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_162", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_163", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_164", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_165", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_166", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_167", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_168", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_169", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_170", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_171", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_172", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_173", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_174", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_175", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_176", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_177", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_178", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_179", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_180", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_181", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_182", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_183", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_184", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_185", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_186", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_187", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_188", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_189", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_190", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_191", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_192", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_193", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_194", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_195", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_196", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_197", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_198", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
{"id": "multi_turn_base_199", "result": "Error during inference: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}", "traceback": "Traceback (most recent call last):\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/_llm_response_generation.py\", line 182, in multi_threaded_inference\n    result, metadata = handler.inference(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 43, in inference\n    return self.inference_multi_turn_FC(\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 164, in inference_multi_turn_FC\n    api_response, query_latency = self._query_FC(inference_data)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 70, in _query_FC\n    return self.generate_with_backoff(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 336, in wrapped_f\n    return copy(f, *args, **kw)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 475, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 376, in iter\n    result = action(retry_state)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/tenacity/__init__.py\", line 478, in __call__\n    result = fn(*args, **kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/utils.py\", line 817, in wrapped\n    return func(*args, **inner_kwargs)\n  File \"/data/jibf/workspace/git_repo/agent_hard_benchmark/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/api_inference/openai_completion.py\", line 48, in generate_with_backoff\n    api_response = self.client.chat.completions.create(**kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\", line 1150, in create\n    return self._post(\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/data/jibf/miniconda3/envs/BFCL/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'detail': '{\"error\": {\"message\": \"The reasoning_content is an intermediate result for display purposes only and will not be included in the context for inference. Please remove the reasoning_content from your message to reduce network traffic.\", \"type\": \"invalid_request_error\", \"param\": null, \"code\": \"invalid_request_error\"}}'}\n"}
